{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4f1fce-1304-43ac-93ee-b46323b5f31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Parameters\n",
    "\n",
    "**Project** CCU056\n",
    "\n",
    "**Description** This notebook defines a set of parameters, which is loaded in each notebook in the data curation pipeline, so that helper functions and parameters are consistently available.\n",
    "\n",
    "**Author(s)** Tom Bolton, Fionna Chalmers, Anna Stevenson (Health Data Science Team, BHF Data Science Centre)\n",
    "\n",
    "**Reviewers** âš  UNREVIEWED\n",
    "\n",
    "**Acknowledgements** Based on CCU004_01-D01-parameters, CCU003_05-D01-parameters and CCU002_07\n",
    "\n",
    "**Notes** This pipeline has an initial production date of 2023-08-15 (`pipeline_production_date` == `2023-08-15`) and the `archived_on` dates used for each dataset correspond to the latest (most recent) batch of data before this date. Should the pipeline and all the notebooks that follow need to be updated and rerun, then this notebook should be rerun directly (before being called by subsequent notebooks) with `pipeline_production_date` updated and `run_all_toggle` switched to True. After this notebook has been rerun the `run_all_toggle` should be reset to False to prevent subsequent notebooks that call this notebook from having to rerun the 'archived_on' section. Rerunning this notebook with the updated `pipeline_production_date` will ensure that the `archived_on` dates used for each dataset are updated with these dates being saved for reference in the collabortion database.\n",
    "\n",
    "**Versions** \n",
    "<br>Version 6 as at '2024-03-07' - study end date also chnaged from 2022-08-01 to 2023-12-01\n",
    "<br>Version 5 as at '2023-11-28' - which will include November provisioning - NICOR datasets still hard-coded as below\n",
    "<br>Version 4 as at '2023-09-15' - issues with HES resolved but NACSA still hardcoded - also hardcoding TAVI back to 2023-03-31 as August batch have no surgery dates\n",
    "<br>Version 3 as at '2023-08-24' - hard coded dates for HES APC, HES APC OTR, NACSA - as recent versions of these have data quality issues\n",
    "<br>Version 2 as at '2023-08-15'\n",
    "<br>Version 1 as at '2023-07-04'\n",
    "\n",
    "**Data Output** \n",
    "- **`ccu056_parameters_df_datasets`**: table of `archived_on` dates for each dataset that can be used consistently throughout the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf28324d-e5ea-4878-8ffa-5694b0b20439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef41313-3ee7-4e93-bcb6-ad7eb558b71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_all_toggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a94c201-6943-4e52-b336-51858df83626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec2080c-46cc-4547-9ccc-8de37eb072d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07cea39e-7765-40ee-890b-7f8fac958cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62278399-3c36-41cc-9af2-ffb7ac66b64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.  Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3ee1f5-d8fd-49a0-9146-bbad9506aa8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Common Functions"
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Repos/shds/common/functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e390892a-b87a-4835-94c7-4a115d25f4ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Help Functions Fionna"
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Repos/shds/Fionna/help_functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e683a2-bb98-4b6b-982d-389f1217167e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de4c3f0-f3f8-4421-8a01-f0b074986bad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "extract_batch_from_archive"
    }
   },
   "outputs": [],
   "source": [
    "# Updated function that compares the number of rows expected (the number that were found when running the parameters notebook in full) against the number of rows observed (the number that were found when extracting the data from the archive in a subsequent notebook). This would alert us to the number of rows being changed in the archive tables, which the data wranglers control.\n",
    "\n",
    "# function to extract the batch corresponding to the pre-defined archived_on date - will be used in subsequent notebooks\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "def extract_batch_from_archive(_df_datasets: DataFrame, _dataset: str):\n",
    "  \n",
    "  # get row from df_archive_tables corresponding to the specified dataset\n",
    "  _row = _df_datasets[_df_datasets['dataset'] == _dataset]\n",
    "  \n",
    "  # check one row only\n",
    "  assert _row.shape[0] != 0, f\"dataset = {_dataset} not found in _df_datasets (datasets = {_df_datasets['dataset'].tolist()})\"\n",
    "  assert _row.shape[0] == 1, f\"dataset = {_dataset} has >1 row in _df_datasets\"\n",
    "  \n",
    "  # create path and extract archived on\n",
    "  _row = _row.iloc[0]\n",
    "  _path = _row['database'] + '.' + _row['table']  \n",
    "  _archived_on = _row['archived_on']\n",
    "  _n_rows_expected = _row['n']  \n",
    "  print(_path + ' (archived_on = ' + _archived_on + ', n_rows_expected = ' + _n_rows_expected + ')')\n",
    "  \n",
    "  # check path exists # commented out for runtime\n",
    "#   _tmp_exists = spark.sql(f\"SHOW TABLES FROM {_row['database']}\")\\\n",
    "#     .where(f.col('tableName') == _row['table'])\\\n",
    "#     .count()\n",
    "#   assert _tmp_exists == 1, f\"path = {_path} not found\"\n",
    "\n",
    "  # extract batch\n",
    "  _tmp = spark.table(_path)\\\n",
    "    .where(f.col('archived_on') == _archived_on)  \n",
    "  \n",
    "  # check number of records returned\n",
    "  _n_rows_observed = _tmp.count()\n",
    "  print(f'  n_rows_observed = {_n_rows_observed:,}')\n",
    "  assert _n_rows_observed > 0, f\"_n_rows_observed == 0\"\n",
    "  assert f'{_n_rows_observed:,}' == _n_rows_expected, f\"_n_rows_observed != _n_rows_expected ({_n_rows_observed:,} != {_n_rows_expected})\"\n",
    "\n",
    "  # return dataframe\n",
    "  return _tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d52ccfc-3f2e-4cd3-8a97-f7447f07ddf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Paths and Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433bfb5a-c41d-4724-bd30-77379dd1c4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 Set Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da5d8f6-92dc-45de-902d-d79a29ad7e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Please set and check the variables below\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Pipeline production date\n",
    "# -----------------------------------------------------------------------------\n",
    "# date at which pipeline was created and archived_on dates for datasets have been selected based on\n",
    "pipeline_production_date = '2024-03-07'\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Databases\n",
    "# -----------------------------------------------------------------------------\n",
    "db = 'dars_nic_391419_j3w9t'\n",
    "dbc = f'{db}_collab'\n",
    "dsa = f'dsa_391419_j3w9t_collab'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Project\n",
    "# -----------------------------------------------------------------------------\n",
    "proj = 'ccu056'\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dates\n",
    "# -----------------------------------------------------------------------------\n",
    "study_start_date = '2000-01-01'\n",
    "study_end_date   = '2023-12-01' #currently set at pipeline productiondate\n",
    "cohort = 'c01'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Datasets\n",
    "# -----------------------------------------------------------------------------\n",
    "# data frame of datasets\n",
    "datasets = [\n",
    "  # -----------------------------------------------------------------------------\n",
    "  # Datasets requested by the project\n",
    "  # -----------------------------------------------------------------------------  \n",
    "    ['gdppr',         dbc, f'gdppr_{db}_archive',              'NHS_NUMBER_DEID',                'DATE']  \n",
    "  , ['hes_apc',       dbc, f'hes_apc_all_years_archive',       'PERSON_ID_DEID',                 'EPISTART']\n",
    "  , ['hes_apc_otr',       dbc, f'hes_apc_otr_all_years_archive', 'PERSON_ID_DEID',                 '']\n",
    "  , ['deaths',        dbc, f'deaths_{db}_archive',             'DEC_CONF_NHS_NUMBER_CLEAN_DEID', 'REG_DATE_OF_DEATH']\n",
    "  , ['nacsa',         dbc, f'nicor_acs_combined_{db}_archive', 'PERSON_ID_DEID',                 'DATE_AND_TIME_OF_OPERATION']\n",
    "  , ['tavi',          dbc, f'nicor_tavi_{db}_archive',         'PERSON_ID_DEID',                 '7_01_DATE_AND_TIME_OF_OPERATION']\n",
    " \n",
    "  \n",
    "  # -----------------------------------------------------------------------------\n",
    "  # Additonal datasets needed for the data curation pipeline for this project\n",
    "  # -----------------------------------------------------------------------------\n",
    "  , ['hes_ae',        dbc, f'hes_ae_all_years_archive',       'PERSON_ID_DEID',                 'ARRIVALDATE']\n",
    "  , ['hes_op',        dbc, f'hes_op_all_years_archive',       'PERSON_ID_DEID',                 'APPTDATE']\n",
    "  , ['hes_cc',        dbc, f'hes_cc_all_years_archive',       'PERSON_ID_DEID',                 'CCSTARTDATE'] \n",
    "  , ['chess',         dbc, f'chess_{db}_archive',             'PERSON_ID_DEID',                 'InfectionSwabDate']\n",
    "  \n",
    "  # -----------------------------------------------------------------------------\n",
    "  # Datasets not required for this project\n",
    "  # -----------------------------------------------------------------------------\n",
    "#   , ['pmeds',         dbc, f'primary_care_meds_{db}_archive', 'Person_ID_DEID',                 'ProcessingPeriodDate']           \n",
    "#   , ['sgss',          dbc, f'sgss_{db}_archive',              'PERSON_ID_DEID',                 'Specimen_Date']\n",
    "#   , ['sus',           dbc, f'sus_{db}_archive',               'NHS_NUMBER_DEID',                'EPISODE_START_DATE'] \n",
    "#   , ['icnarc',        dbc, f'icnarc_{db}_archive',            'NHS_NUMBER_DEID',               'Date_of_admission_to_your_unit']  \n",
    "#   , ['ssnap',         dbc, f'ssnap_{db}_archive',             'PERSON_ID_DEID',                'S1ONSETDATETIME'] \n",
    "#   , ['minap',         dbc, f'minap_{db}_archive',             'NHS_NUMBER_DEID',               'ARRIVAL_AT_HOSPITAL'] \n",
    "#   , ['nhfa',          dbc, f'nhfa_{db}_archive',              '1_03_NHS_NUMBER_DEID',          '2_00_DATE_OF_VISIT'] \n",
    "#   , ['nvra',          dbc, f'nvra_{db}_archive',              'NHS_NUMBER_DEID',               'DATE'] \n",
    "#   , ['vacc',          dbc, f'vaccine_status_{db}_archive',    'PERSON_ID_DEID',                'DATE_AND_TIME']  \n",
    "]\n",
    "\n",
    "tmp_df_datasets = pd.DataFrame(datasets, columns=['dataset', 'database', 'table', 'id', 'date']).reset_index()\n",
    "\n",
    "if(run_all_toggle):\n",
    "  print('tmp_df_datasets:\\n', tmp_df_datasets.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d88639-a5be-45f0-b67c-539153784ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 Datasets Archived States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7694c915-0cca-40b5-b4b4-9a53691917ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2.1 Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3d3b6d-a77c-489a-98ea-71cf1aab2b41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "All archived states"
    }
   },
   "outputs": [],
   "source": [
    "# for each dataset in tmp_df_datasets, \n",
    "#   tabulate all archived_on dates (for information)\n",
    "#   find the latest (most recent) archived_on date before the pipeline_production_date\n",
    "#   create a table containing a row with the latest archived_on date and count of the number of records for each dataset\n",
    "  \n",
    "# this will not run each time the Parameters notebook is run in annother notebook - will only run if the toggle is switched to True\n",
    "if(run_all_toggle):\n",
    "\n",
    "  latest_archived_on = []\n",
    "  lsoa_1st = []\n",
    "  for index, row in tmp_df_datasets.iterrows():\n",
    "    # initial  \n",
    "    dataset = row['dataset']\n",
    "    path = row['database'] + '.' + row['table']\n",
    "    print(index, dataset, path); print()\n",
    "\n",
    "    # point to table\n",
    "    tmpd = spark.table(path)\n",
    "\n",
    "    # tabulate all archived_on dates\n",
    "    tmpt = tab(tmpd, 'archived_on')\n",
    "    \n",
    "    # extract latest (most recent) archived_on date before the pipeline_production_date\n",
    "    tmpa = (\n",
    "      tmpd\n",
    "      .groupBy('archived_on')\n",
    "      .agg(f.count(f.lit(1)).alias('n'))\n",
    "      .withColumn('n', f.format_number('n', 0))\n",
    "      .where(f.col('archived_on') <= pipeline_production_date)\n",
    "      .orderBy(f.desc('archived_on'))\n",
    "      .limit(1)\n",
    "      .withColumn('dataset', f.lit(dataset))\n",
    "      .select('dataset', 'archived_on', 'n')\n",
    "    )\n",
    "    \n",
    "    # extract closest archived_on date that comes after study_start_date\n",
    "    if(dataset==\"gdppr\"):\n",
    "      tmpb = (\n",
    "        tmpd\n",
    "        .groupBy('archived_on')\n",
    "        .agg(f.count(f.lit(1)).alias('n'))\n",
    "        .withColumn('n', f.format_number('n', 0))\n",
    "        .where(f.col('archived_on') >= study_start_date)\n",
    "        .orderBy(f.asc('archived_on'))\n",
    "        .limit(1)\n",
    "        .withColumn('dataset', f.lit(dataset))\n",
    "        .select('dataset', 'archived_on', 'n')\n",
    "      )\n",
    "      \n",
    "      if(index == 0): lsoa_1st = tmpb\n",
    "      else: lsoa_1st = lsoa_1st.unionByName(tmpb)\n",
    "    \n",
    "    # append results\n",
    "    if(index == 0): latest_archived_on = tmpa\n",
    "    else: latest_archived_on = latest_archived_on.unionByName(tmpa)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "  # check\n",
    "  print('Latest (most recent) archived_on date before pipeline_production_date')\n",
    "  print(latest_archived_on.toPandas().to_string())\n",
    "  print('\\nClosest (1st) GDPPR archived_on date following study_start_date')\n",
    "  print(lsoa_1st.toPandas().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dad7bfb-7a0d-4e92-be6d-354ed89bb98f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2.2 Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a7e7d4-877d-4838-86c3-4c83eaa3c936",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Latest archived states"
    }
   },
   "outputs": [],
   "source": [
    "# this will not run each time the Parameters notebook is run in annother notebook - will only run if the toggle is switched to True\n",
    "if(run_all_toggle):\n",
    "  # check\n",
    "  display(latest_archived_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595a20a9-72f5-425a-bf89-a79ec6997d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2.3 Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a020527-e193-494f-91f7-fa3a42a5707e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare the tables to be saved\n",
    "\n",
    "# this will not run each time the Parameters notebook is run in annother notebook - will only run if the toggle is switched to True\n",
    "if(run_all_toggle):\n",
    "  \n",
    "  # merge the datasets dataframe with the latest_archived_on\n",
    "  tmp_df_datasets_sp = spark.createDataFrame(tmp_df_datasets) \n",
    "  parameters_df_datasets = merge(tmp_df_datasets_sp, latest_archived_on, ['dataset'], validate='1:1', assert_results=['both'], indicator=0).orderBy('index'); print()\n",
    "  \n",
    "  # check  \n",
    "  print(parameters_df_datasets.toPandas().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b46bb6c-3ef6-431c-8620-71dc09d93659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2.4 Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f72fe4-8530-4b14-8e63-bea5bae88e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adhoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aaf9f89-0643-4df4-af47-13b177dd24d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are issues with data being missing from HES APC, HES APC OTR and NACSA.\n",
    "\n",
    "Will hard code some of these here for time being, to get datasets as at the same archived_on date to align for HES APC, and the best version of NACSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf244ad-e746-4d08-b0a7-e18f463ed02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hes_apc_all = spark.table(f'dars_nic_391419_j3w9t_collab.hes_apc_all_years_archive')\n",
    "# display(hes_apc_all.select(\"archived_on\").groupBy(\"archived_on\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ada12d-7374-4d11-a3ba-d75226a1464d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hes_apc_otr_all = spark.table(f'dars_nic_391419_j3w9t_collab.hes_apc_otr_all_years_archive')\n",
    "# display(hes_apc_otr_all.select(\"archived_on\").groupBy(\"archived_on\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20fd61f2-eda7-4e97-84c9-12b48afc409c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hes_apc_otr_all = spark.table(f'dars_nic_391419_j3w9t_collab.hes_apc_otr_all_years_archive')\n",
    "# display(hes_apc_otr_all.select(\"archived_on\").groupBy(\"archived_on\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43b1427-5f59-420a-9f62-a45a7e46cf1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nacsa = spark.table(f'dars_nic_391419_j3w9t_collab.nicor_acs_combined_dars_nic_391419_j3w9t_archive')\n",
    "# display(nacsa.select(\"archived_on\").groupBy(\"archived_on\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de2e9e2-da34-4aa8-8a1b-7f321d090183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tavi = spark.table(f'dars_nic_391419_j3w9t_collab.nicor_tavi_dars_nic_391419_j3w9t_archive')\n",
    "# display(tavi.select(\"archived_on\").groupBy(\"archived_on\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98248f1-ff43-4206-94f1-4aa562250007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "HES APC and HES APC OPT \"2023-04-27\" <br>\n",
    "NICOR NACSA will use \"2023-03-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b922d230-e9e2-4bb3-a99a-66dfb9cec510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hes_apc_count = (\n",
    "#     spark.table(f'dars_nic_391419_j3w9t_collab.hes_apc_all_years_archive')\n",
    "#                 .filter(f.col(\"archived_on\")==\"2023-04-27\").count()\n",
    "#     )\n",
    "\n",
    "# hes_apc_otr_count = (\n",
    "#     spark.table(f'dars_nic_391419_j3w9t_collab.hes_apc_otr_all_years_archive')\n",
    "#                 .filter(f.col(\"archived_on\")==\"2023-04-27\").count()\n",
    "#     )\n",
    "\n",
    "nacsa_count = (\n",
    "    spark.table(f'dars_nic_391419_j3w9t_collab.nicor_acs_combined_dars_nic_391419_j3w9t_archive')\n",
    "    .filter(f.col(\"archived_on\")==\"2023-03-31\").count()\n",
    ")\n",
    "\n",
    "\n",
    "tavi_count = (\n",
    "    spark.table(f'dars_nic_391419_j3w9t_collab.nicor_tavi_dars_nic_391419_j3w9t_archive')\n",
    "    .filter(f.col(\"archived_on\")==\"2023-03-31\").count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7384fb3e-5e6a-4b1a-a75b-ef6e658bbcf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if(run_all_toggle):\n",
    "    parameters_df_datasets = (\n",
    "        parameters_df_datasets\n",
    "        # .withColumn(\"archived_on\", f.when(f.col(\"dataset\")==\"hes_apc\",\"2023-04-27\").otherwise(f.col(\"archived_on\")))\n",
    "        # .withColumn(\"archived_on\", f.when(f.col(\"dataset\")==\"hes_apc_otr\",\"2023-04-27\").otherwise(f.col(\"archived_on\")))\n",
    "        .withColumn(\"archived_on\", f.when(f.col(\"dataset\")==\"nacsa\",\"2023-03-31\").otherwise(f.col(\"archived_on\")))\n",
    "        .withColumn(\"archived_on\", f.when(f.col(\"dataset\")==\"tavi\",\"2023-03-31\").otherwise(f.col(\"archived_on\")))\n",
    "\n",
    "        # .withColumn(\"n\", f.when(f.col(\"dataset\")==\"hes_apc\",hes_apc_count).otherwise(f.col(\"n\")))\n",
    "        # .withColumn(\"n\", f.when(f.col(\"dataset\")==\"hes_apc_otr\",hes_apc_otr_count).otherwise(f.col(\"n\")))\n",
    "        .withColumn(\"n\", f.when(f.col(\"dataset\")==\"nacsa\",nacsa_count).otherwise(f.col(\"n\")))\n",
    "        .withColumn(\"n\", f.when(f.col(\"dataset\")==\"tavi\",tavi_count).otherwise(f.col(\"n\")))\n",
    "        \n",
    "        .withColumn(\"n\", f.when(f.col(\"dataset\").isin([\n",
    "            #'hes_apc','hes_apc_otr',\n",
    "            'nacsa','tavi']), f.format_number(f.col(\"n\").cast(\"int\"),0)).otherwise(f.col('n')))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52cb8e0-875a-4087-a67d-be84016134e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save the parameters_df_datasets table\n",
    "# which we can simply import below when not running all and calling this notebook in subsequent notebooks\n",
    "\n",
    "# this will not run each time the Parameters notebook is run in annother notebook - will only run if the toggle is switched to True\n",
    "if(run_all_toggle):\n",
    "  save_table(df=parameters_df_datasets, out_name=f'{proj}_parameters_df_datasets', save_previous=True, data_base=dsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20e1f68-7158-4b27-be81-15a3d8f98420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2.5 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c7a807-bee4-455a-bc76-f82dc1eb00fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import the parameters_df_datasets table \n",
    "# convert to a Pandas dataframe and transform archived_on to a string (to conform to the input that the extract_batch_from_archive function is expecting)\n",
    "\n",
    "spark.sql(f'REFRESH TABLE {dsa}.{proj}_parameters_df_datasets')\n",
    "parameters_df_datasets = (\n",
    "  spark.table(f'{dsa}.{proj}_parameters_df_datasets')\n",
    "  .orderBy('index')\n",
    "  .toPandas()\n",
    ")\n",
    "parameters_df_datasets['archived_on'] = parameters_df_datasets['archived_on'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8986b4a-f166-45e6-9ccb-7a3d13b56c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parameters_df_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e6df94-f3e2-464b-b459-3f5b2864e659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 Curated Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9c7486-f59f-4c90-9979-12d5db7015b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# These are paths to data tables curated in subsequent notebooks that may be\n",
    "# needed in subsequent notebooks from which they were curated\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# note: the below is largely listed in order of appearance within the pipeline:\n",
    "\n",
    "# temp path for TAVI as using Live version instead of an archived version for now\n",
    "path_tavi = f'{db}.nicor_tavi_{db}'\n",
    "\n",
    "# reference tables\n",
    "path_ref_bhf_phenotypes  = 'bhf_cvd_covid_uk_byod.bhf_covid_uk_phenotypes_20210127'\n",
    "path_ref_geog            = 'dss_corporate.ons_chd_geo_listings'\n",
    "path_ref_imd             = 'dss_corporate.english_indices_of_dep_v02'\n",
    "path_ref_gp_refset       = 'dss_corporate.gpdata_snomed_refset_full'\n",
    "path_ref_gdppr_refset    = 'dss_corporate.gdppr_cluster_refset'\n",
    "path_ref_icd10           = 'dss_corporate.icd10_group_chapter_v01'\n",
    "path_ref_opcs4           = 'dss_corporate.opcs_codes_v02'\n",
    "# path_ref_map_ctv3_snomed = 'dss_corporate.read_codes_map_ctv3_to_snomed'\n",
    "# path_ref_ethnic_hes      = 'dss_corporate.hesf_ethnicity'\n",
    "# path_ref_ethnic_gdppr    = 'dss_corporate.gdppr_ethnicity'\n",
    "\n",
    "# curated tables\n",
    "path_cur_hes_apc_long      = f'{dsa}.{proj}_cur_hes_apc_all_years_archive_long'\n",
    "path_cur_hes_apc_op_long   = f'{dsa}.{proj}_cur_hes_apc_all_years_archive_op_long'\n",
    "path_cur_deaths_long       = f'{dsa}.{proj}_cur_deaths_{db}_archive_long'\n",
    "path_cur_deaths_sing       = f'{dsa}.{proj}_cur_deaths_{db}_archive_sing'\n",
    "path_cur_lsoa_region       = f'{dsa}.{proj}_cur_lsoa_region_lookup'\n",
    "path_cur_lsoa_imd          = f'{dsa}.{proj}_cur_lsoa_imd_lookup'\n",
    "path_cur_lsoa              = f'{dsa}.{proj}_lsoa'\n",
    "\n",
    "# path_cur_vacc_first        = f'{dsa}.{proj}_cur_vacc_first'\n",
    "# path_cur_covid             = f'{dsa}.{proj}_cur_covid'\n",
    "\n",
    "# # temporary tables\n",
    "path_tmp_skinny_unassembled             = f'{dsa}.{proj}_tmp_kpc_harmonised'\n",
    "path_tmp_skinny_assembled               = f'{dsa}.{proj}_tmp_kpc_selected'\n",
    "path_tmp_skinny                         = f'{dsa}.{proj}_tmp_skinny'\n",
    "\n",
    "path_tmp_quality_assurance_hx_1st_wide  = f'{dsa}.{proj}_tmp_quality_assurance_hx_1st_wide'\n",
    "path_tmp_quality_assurance_hx_1st       = f'{dsa}.{proj}_tmp_quality_assurance_hx_1st'\n",
    "path_tmp_quality_assurance_qax          = f'{dsa}.{proj}_tmp_quality_assurance_qax'\n",
    "path_tmp_quality_assurance              = f'{dsa}.{proj}_tmp_quality_assurance'\n",
    "\n",
    "path_tmp_inc_exc_cohort                 = f'{dsa}.{proj}_tmp_inc_exc_cohort'\n",
    "path_tmp_inc_exc_flow                   = f'{dsa}.{proj}_tmp_inc_exc_flow'\n",
    "\n",
    "path_tmp_hx_af_hyp_cohort               = f'{dsa}.{proj}_tmp_hx_af_hyp_cohort'\n",
    "path_tmp_hx_af_hyp_gdppr                = f'{dsa}.{proj}_tmp_hx_af_hyp_gdppr'\n",
    "path_tmp_hx_af_hyp_hes_apc              = f'{dsa}.{proj}_tmp_hx_af_hyp_hes_apc'\n",
    "path_tmp_hx_af_hyp                      = f'{dsa}.{proj}_tmp_hx_af_hyp'\n",
    "\n",
    "path_tmp_hx_nonfatal                    = f'{dsa}.{proj}_tmp_hx_nonfatal'\n",
    "\n",
    "path_tmp_inc_exc_2_cohort                 = f'{dsa}.{proj}_tmp_inc_exc_2_cohort'\n",
    "path_tmp_inc_exc_2_flow                   = f'{dsa}.{proj}_tmp_inc_exc_2_flow'\n",
    "\n",
    "# path_tmp_covariates_hes_apc             = f'{dsa}.{proj}_tmp_covariates_hes_apc'\n",
    "# path_tmp_covariates_pmeds               = f'{dsa}.{proj}_tmp_covariates_pmeds'\n",
    "# path_tmp_covariates_lsoa                = f'{dsa}.{proj}_tmp_covariates_lsoa'\n",
    "# path_tmp_covariates_lsoa_2              = f'{dsa}.{proj}_tmp_covariates_lsoa_2'\n",
    "# path_tmp_covariates_lsoa_3              = f'{dsa}.{proj}_tmp_covariates_lsoa_3'\n",
    "# path_tmp_covariates_n_consultations     = f'{dsa}.{proj}_tmp_covariates_n_consultations'\n",
    "# path_tmp_covariates_unique_bnf_chapters = f'{dsa}.{proj}_tmp_covariates_unique_bnf_chapters'\n",
    "# path_tmp_covariates_hx_out_1st_wide     = f'{dsa}.{proj}_tmp_covariates_hx_out_1st_wide'\n",
    "# path_tmp_covariates_hx_com_1st_wide     = f'{dsa}.{proj}_tmp_covariates_hx_com_1st_wide'\n",
    "\n",
    "# out tables\n",
    "path_out_codelist_quality_assurance      = f'{dsa}.{proj}_out_codelist_quality_assurance'\n",
    "path_out_codelist_cvd                    = f'{dsa}.{proj}_out_codelist_cvd'\n",
    "# path_out_codelist_comorbidity            = f'{dsa}.{proj}_out_codelist_comorbidity'\n",
    "path_out_codelist_covid                  = f'{dsa}.{proj}_out_codelist_covid'\n",
    "path_out_codelist_covariates             = f'{dsa}.{proj}_out_codelist_covariates'\n",
    "path_out_codelist_covariates_markers     = f'{dsa}.{proj}_out_codelist_covariates_markers'\n",
    "path_out_codelist_outcomes               = f'{dsa}.{proj}_out_codelist_outcomes'\n",
    "\n",
    "path_out_cohort                          = f'{dsa}.{proj}_out_cohort'\n",
    "\n",
    "# path_out_covariates                 = f'{dsa}.{proj}_out_covariates'\n",
    "path_out_exposures                    = f'{dsa}.{proj}_out_exposures'\n",
    "path_out_outcomes                     = f'{dsa}.{proj}_out_outcomes'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CCU056-01-parameters",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}